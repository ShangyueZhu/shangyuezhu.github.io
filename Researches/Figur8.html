<html>
	<head>
		<link href="style.css" rel="stylesheet">
		<title>Shangyue Zhu resume</title>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
	</head>

	<body>
		<header id="header">
			<!-- resume header with your name and title -->
			<h1>Welcome to <b>Shangyue</b>'s Blog</h1>
			<hr>
    		<p1><a href="https://shangyuezhu.github.io/">Home</a></p1>   |   <p1><a href="/research.html">Research</a></p1>   | <p1><a href="/public.html">Public</a> </p1>
			<hr>
		</header>
		
        <article id="mainLeft">
            <section>
            <h2>Jumping Motion Recognition</h2>
            <br>

            <h3><b>Research Description</b></h3>
            <p>This research was developed at the <a href="https://figur8tech.com/">Figur8. Inc</a>
            to identify the motion state of people when they jump by detecting data from wearable sensors.
            The problem of the research can be simplified to find the data that possesses 
            the characteristic amount of jumping in a string of time sequence data.
            The purpose of this research is to extract the motion information when jumping, such as jumping
            up and down. The data comes from a specific muscle sensor, which records muscle stretch during
            jumping. Through Recurrent Neural Network modeling, the movement characteristics of the jump
            are separated, and extracted the jump information to recognize the specific activities.  </p>
                 
            <br><br>
              
			<h3><b>Research Method</b></h3>
            
            <img src="/Images/Figur8/jump.png" width="450" height="350" />
            <img src="/Images/Figur8/model.png" width="400" height="350" />
			<p>
                As shown on the left of the figure, according to the recorded video, 
                mark the video of the jumping part according to the time point.
                In the modeling process, the LSTM of the RNN is considered as the best model,
                because the collected data has the characteristics of time series, 
                and the tested LSTM mode is easier to identify the information of motion 
                in the time series. Model information is shown on the right. 
            </p>
            <ul>    
                <!-- <a href="/Researches/Figur8.html"></a> -->
                <li>Labeling data. Based on the recorded video marker jumps, label the data of the jump segment as 1 and the rest as 0.</li>
                <li>Build the model and training. The reshaped data is normalized and put into the RNN network for training.</li>
            </ul>
            
			
            <br>
            <br>
            
            <h3><b>RNN Test Results</b></h3>
            <img src="/Images/Figur8/test1.png" width="200" height="200" />
            <img src="/Images/Figur8/test2.png" width="200" height="200" />
            <img src="/Images/Figur8/test3.png" width="200" height="200" />
            <img src="/Images/Figur8/test4.png" width="200" height="200" />

            <h3><b>Summary</b></h3>
            </section>

        </article>
        
		
			
		
	</body>
</html>